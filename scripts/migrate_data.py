import asyncio
import sqlite3
import asyncpg
import os
import json
from dotenv import load_dotenv
import logging

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Load Environment Variables
load_dotenv()

SQLITE_PATH = "data/database.db"
PG_HOST = os.getenv("DB_HOST", "localhost")
PG_PORT = os.getenv("DB_PORT", "5432")
PG_DB = os.getenv("DB_NAME", "discord_bot_db")
PG_USER = os.getenv("DB_USER", "discord_bot")
PG_PASS = os.getenv("DB_PASS", "discord_bot_password")

# Type Mapping: SQLite -> PostgreSQL
TYPE_MAPPING = {
    "INTEGER": "BIGINT",
    "TEXT": "TEXT",
    "REAL": "DOUBLE PRECISION",
    "BLOB": "BYTEA",
    "BOOLEAN": "BOOLEAN",
    "DATETIME": "TIMESTAMP"
}

async def create_table_from_schema(pg_conn, table_name, columns):
    """Create table in Postgres based on SQLite columns."""
    col_defs = []
    pk_cols = []
    
    for col in columns:
        cid, name, dtype, notnull, dflt_value, pk = col
        # Normalize dtype
        dtype = dtype.upper().split("(")[0] 
        pg_type = TYPE_MAPPING.get(dtype, "TEXT")
        
        # JSONB Heuristic: If column name suggests JSON, use JSONB
        if "metadata" in name.lower() or "config" in name.lower() or "data" in name.lower():
             pg_type = "JSONB"
        
        col_def = f'"{name}" {pg_type}'
        
        # Relax constraints for migration (Postgres is stricter than SQLite)
        # if notnull:
        #    col_def += " NOT NULL"
        # 
        # col_defs.append(col_def)  <-- REMOVED THIS LINE
        
        if pk > 0:
            pk_cols.append(name)
            
            # Auto-increment for Integer PKs (mimic SQLite)
            if dtype == "INTEGER" or pg_type == "BIGINT":
                 # Replace BIGINT with serial type
                 col_def = f'"{name}" BIGINT GENERATED BY DEFAULT AS IDENTITY'
                 # Note: "PRIMARY KEY" will be added at the end, so valid syntax is:
                 # col BIGINT GENERATED ...
                 # CONSTRAINT ... PRIMARY KEY (col)
        
        if notnull:
            # col_def += " NOT NULL" # Relaxed
            pass
        
        col_defs.append(col_def)
    
    # Add Primary Key Constraint
    if pk_cols:
        pk_str = ", ".join([f'"{c}"' for c in pk_cols])
        col_defs.append(f'PRIMARY KEY ({pk_str})')
    
    # Clean start: Drop table if exists
    logger.info(f"Dropping table {table_name}...")
    await pg_conn.execute(f'DROP TABLE IF EXISTS "{table_name}" CASCADE')
    
    create_sql = f'CREATE TABLE "{table_name}" ({", ".join(col_defs)});'
    logger.info(f"Creating table {table_name}: {create_sql}")
    await pg_conn.execute(create_sql)

async def copy_data(sqlite_cursor, pg_conn, table_name, columns):
    """Copy data from SQLite to Postgres."""
    from datetime import datetime
    
    # Get column definitions
    col_names = [col[1] for col in columns]
    pk_indices = [i for i, col in enumerate(columns) if col[5] > 0] # col[5] is pk flag
    col_types = []
    
    for col in columns:
        dtype = col[2].upper().split("(")[0]
        if "metadata" in col[1].lower() or "config" in col[1].lower() or "data" in col[1].lower():
             pg_type = "JSONB"
        else:
             pg_type = TYPE_MAPPING.get(dtype, "TEXT")
        col_types.append(pg_type)

    col_str = ", ".join([f'"{c}"' for c in col_names])
    placeholders = ", ".join([f"${i+1}" for i in range(len(col_names))])
    
    # Read from SQLite
    logger.info(f"Reading data from {table_name}...")
    sqlite_cursor.execute(f'SELECT * FROM "{table_name}"')
    rows = sqlite_cursor.fetchall()
    
    if not rows:
        logger.info(f"Table {table_name} is empty. Skipping copy.")
        return

    # Bulk Insert into Postgres
    cleaned_rows = []
    skipped_count = 0
    
    for row in rows:
        clean_row = list(row)
        skip_row = False
        
        # 1. Sanitize Data Types
        for i, val in enumerate(clean_row):
            target_type = col_types[i]
            
            # Check PK Nulls
            if i in pk_indices and val is None:
                # Postgres PK cannot be null. Skip valid data.
                skip_row = True
                break
            
            # Handle TIMESTAMP
            if target_type == "TIMESTAMP" and isinstance(val, str):
                try:
                    dt = datetime.fromisoformat(val)
                    clean_row[i] = dt.replace(tzinfo=None)
                except ValueError:
                    logger.warning(f"Bad timestamp '{val}' in {table_name}.{col_names[i]}")
                    clean_row[i] = None
            
            # Handle BOOLEAN
            if target_type == "BOOLEAN" and isinstance(val, int):
                clean_row[i] = bool(val)
        
        if skip_row:
            skipped_count += 1
            continue
            
        cleaned_rows.append(tuple(clean_row))

    if skipped_count > 0:
        logger.warning(f"Skipped {skipped_count} rows in {table_name} due to NULL Primary Keys.")

    if not cleaned_rows:
        logger.info(f"No valid rows to insert for {table_name}.")
        return

    insert_sql = f'INSERT INTO "{table_name}" ({col_str}) VALUES ({placeholders}) ON CONFLICT DO NOTHING'
    
    try:
        await pg_conn.executemany(insert_sql, cleaned_rows)
        logger.info(f"Successfully copied {len(cleaned_rows)} rows to {table_name}.")
    except Exception as e:
        logger.error(f"Failed to copy data for {table_name}: {e}")
        # Log first few rows to debug
        # logger.error(f"Sample data: {cleaned_rows[:1]}")
        raise e

async def main():
    logger.info("Starting Migration...")
    
    # 1. Connect to SQLite
    if not os.path.exists(SQLITE_PATH):
        logger.error(f"SQLite file not found: {SQLITE_PATH}")
        return
    
    sqlite_conn = sqlite3.connect(SQLITE_PATH)
    sqlite_cursor = sqlite_conn.cursor()
    
    # 2. Connect to Postgres
    try:
        pg_conn = await asyncpg.connect(
            host=PG_HOST, port=PG_PORT, user=PG_USER, password=PG_PASS, database=PG_DB
        )
        logger.info("Connected to PostgreSQL.")
    except Exception as e:
        logger.error(f"Failed to connect to Postgres: {e}")
        return

    try:
        # 3. Get list of tables
        sqlite_cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
        tables = sqlite_cursor.fetchall()
        
        for (table_name,) in tables:
            logger.info(f"Processing table: {table_name}")
            
            # Get Schema
            sqlite_cursor.execute(f"PRAGMA table_info(\"{table_name}\")")
            columns = sqlite_cursor.fetchall()
            
            # Create Table in Postgres
            await create_table_from_schema(pg_conn, table_name, columns)
            
            # Copy Data
            await copy_data(sqlite_cursor, pg_conn, table_name, columns)
            
            # Reset Sequences for Integer PKs
            for col in columns:
                if col[5] > 0: # PK
                     dtype = col[2].upper().split("(")[0]
                     if dtype == "INTEGER":
                         pk_col = col[1]
                         logger.info(f"Resetting sequence for {table_name}.{pk_col}...")
                         try:
                             sql = f"""
                             SELECT setval(
                                 pg_get_serial_sequence('"{table_name}"', '{pk_col}'), 
                                 COALESCE((SELECT MAX("{pk_col}") FROM "{table_name}"), 0) + 1, 
                                 false
                             );
                             """
                             await pg_conn.execute(sql)
                         except Exception as e:
                             logger.warning(f"Could not reset sequence for {table_name}: {e}")
            
            logger.info(f"Finished processing {table_name}.\n")
            
        logger.info("Migration Complete!")
        
    except Exception as e:
        logger.error(f"Migration Failed: {e}", exc_info=True)
    finally:
        await pg_conn.close()
        sqlite_conn.close()

if __name__ == "__main__":
    asyncio.run(main())
